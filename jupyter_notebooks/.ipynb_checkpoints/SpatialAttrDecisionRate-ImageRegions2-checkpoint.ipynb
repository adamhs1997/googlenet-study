{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JndnmDMp66FL"
   },
   "source": [
    "##### Copyright 2018 Google LLC.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "hMqWDc_m6rUC"
   },
   "outputs": [],
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hOBBuzMaxU37"
   },
   "source": [
    "# Install / Import / Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UL1yOZtjqkcj"
   },
   "source": [
    "This code depends on [Lucid](https://github.com/tensorflow/lucid) (our visualization library), and [svelte](https://svelte.technology/) (a web framework). The following cell will install both of them, and dependancies such as TensorFlow. And then import them as appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15298,
     "status": "ok",
     "timestamp": 1539449536510,
     "user": {
      "displayName": "Adam Horvath-Smith",
      "photoUrl": "",
      "userId": "02503422148139327677"
     },
     "user_tz": 240
    },
    "id": "AA17rJBLuyYH",
    "outputId": "4107bb2a-9a1a-44cd-d97e-42049a84cd8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/w266ajh/.local/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:104: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`normal` is a deprecated alias for `truncated_normal`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/w266ajh/.local/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:104: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`normal` is a deprecated alias for `truncated_normal`\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imsave\n",
    "from os import getcwd, listdir\n",
    "from PIL import Image\n",
    "\n",
    "import lucid.modelzoo.vision_models as models\n",
    "from lucid.misc.io import show\n",
    "import lucid.optvis.render as render\n",
    "from lucid.misc.io import show, load\n",
    "from lucid.misc.io.reading import read\n",
    "from lucid.misc.io.showing import _image_url\n",
    "from lucid.misc.gradient_override import gradient_override_map\n",
    "import lucid.scratch.web.svelte as lucid_svelte\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from top5gen.examples.inception_pretrained import get_top_five"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0cUPBCRyG9xE"
   },
   "source": [
    "# Attribution Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FWHqimIqk2Bs"
   },
   "outputs": [],
   "source": [
    "# This, obviously, loads up the model\n",
    "model = models.InceptionV1()\n",
    "model.load_graphdef()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 381,
     "status": "ok",
     "timestamp": 1539452363680,
     "user": {
      "displayName": "Adam Horvath-Smith",
      "photoUrl": "",
      "userId": "02503422148139327677"
     },
     "user_tz": 240
    },
    "id": "xIDcG0vjaDtk",
    "outputId": "c0b7cb0b-d043-45ee-acf6-90875a5cd584"
   },
   "outputs": [],
   "source": [
    "# Here we take in the 1000-dimension vector of potential image labels\n",
    "labels_str = read(\"https://gist.githubusercontent.com/aaronpolhamus/964a4411c0906315deb9f4a3723aac57/raw/aa66dd9dbf6b56649fa3fab83659b2acbf3cbfd1/map_clsloc.txt\")\n",
    "labels = [line[line.find(\" \"):].strip().encode() for line in labels_str.decode().split(\"\\n\")]\n",
    "labels = [label.decode().strip().split()[1].replace(\"_\", \" \") for label in labels]\n",
    "labels = [\"dummy\"] + labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p1S73WcbKIdI"
   },
   "outputs": [],
   "source": [
    "def raw_class_spatial_attr(img, layer, label, override=None):\n",
    "  \"\"\"How much did spatial positions at a given layer effect a output class?\"\"\"\n",
    "\n",
    "  # Set up a graph for doing attribution...\n",
    "  with tf.Graph().as_default(), tf.Session(), gradient_override_map(override or {}):\n",
    "    t_input = tf.placeholder_with_default(img, [None, None, 3])\n",
    "    T = render.import_model(model, t_input, t_input)\n",
    "    \n",
    "    # Compute activations\n",
    "    acts = T(layer).eval()\n",
    "    \n",
    "    # display results\n",
    "\n",
    "    \n",
    "    if label is None: return np.zeros(acts.shape[1:-1])\n",
    "    \n",
    "    # Compute gradient\n",
    "    softmax_result = T(\"softmax2_pre_activation\")\n",
    "    score = softmax_result[0, labels.index(label)] # The \"score\" is the softmax result at idx [0, label idx]\n",
    "    t_grad = tf.gradients([score], [T(layer)])[0]   \n",
    "    grad = t_grad.eval({T(layer) : acts})\n",
    "    \n",
    "    # Linear approximation of effect of spatial position\n",
    "    return np.sum(acts * grad, -1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v_jkx5Niji4Q"
   },
   "outputs": [],
   "source": [
    "def raw_spatial_spatial_attr(img, layer1, layer2, override=None):\n",
    "  \"\"\"Attribution between spatial positions in two different layers.\"\"\"\n",
    "\n",
    "  # Set up a graph for doing attribution...\n",
    "  with tf.Graph().as_default(), tf.Session(), gradient_override_map(override or {}):\n",
    "    t_input = tf.placeholder_with_default(img, [None, None, 3])\n",
    "    T = render.import_model(model, t_input, t_input)\n",
    "    \n",
    "    # Compute activations\n",
    "    acts1 = T(layer1).eval()\n",
    "    acts2 = T(layer2).eval({T(layer1) : acts1})\n",
    "    \n",
    "    # Construct gradient tensor\n",
    "    # Backprop from spatial position (n_x, n_y) in layer2 to layer1.\n",
    "    n_x, n_y = tf.placeholder(\"int32\", []), tf.placeholder(\"int32\", [])\n",
    "    layer2_mags = tf.sqrt(tf.reduce_sum(T(layer2)**2, -1))[0]\n",
    "    score = layer2_mags[n_x, n_y]\n",
    "    t_grad = tf.gradients([score], [T(layer1)])[0]\n",
    "    \n",
    "    # Compute attribution backwards from each positin in layer2\n",
    "    attrs = []\n",
    "    for i in range(acts2.shape[1]):\n",
    "      attrs_ = []\n",
    "      for j in range(acts2.shape[2]):\n",
    "        grad = t_grad.eval({n_x : i, n_y : j, T(layer1) : acts1})\n",
    "        # linear approximation of imapct\n",
    "        attr = np.sum(acts1 * grad, -1)[0]\n",
    "        attrs_.append(attr)\n",
    "      attrs.append(attrs_)\n",
    "  return np.asarray(attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ku6hGbYmiQNI"
   },
   "source": [
    "# Spatial Attribution Interface\n",
    "\n",
    "In this section, we build the *interface* for interacting with the different kinds of spatial attribution data that we can compute using the above functions. Feel free to skip over this if you aren't interested in that part. The main reason we're including it is so that you can change the interface if you want to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1026,
     "status": "ok",
     "timestamp": 1539449544828,
     "user": {
      "displayName": "Adam Horvath-Smith",
      "photoUrl": "",
      "userId": "02503422148139327677"
     },
     "user_tz": 240
    },
    "id": "X6TFCwbQhre2",
    "outputId": "dfc9c7c0-0330-4c21-c1f6-45846d19ef06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to build svelte component from html...\n",
      "svelte compile --format iife /tmp/svelte_9djg85dw/SpatialWidget_f3f7f8db_f07a_418e_af83_c1074d6edd0b.html > /tmp/svelte_9djg85dw/SpatialWidget_f3f7f8db_f07a_418e_af83_c1074d6edd0b.js\n",
      "b'svelte version 1.64.1\\ncompiling ../../../../tmp/svelte_9djg85dw/SpatialWidget_f3f7f8db_f07a_418e_af83_c1074d6edd0b.html...\\n(4:4) \\xe2\\x80\\x93 A11y: <img> element should have an alt attribute\\n(5:4) \\xe2\\x80\\x93 A11y: <img> element should have an alt attribute\\n(11:4) \\xe2\\x80\\x93 A11y: <img> element should have an alt attribute\\n(12:4) \\xe2\\x80\\x93 A11y: <img> element should have an alt attribute\\n(18:4) \\xe2\\x80\\x93 A11y: <img> element should have an alt attribute\\n(19:4) \\xe2\\x80\\x93 A11y: <img> element should have an alt attribute\\n(25:4) \\xe2\\x80\\x93 A11y: <img> element should have an alt attribute\\n(26:4) \\xe2\\x80\\x93 A11y: <img> element should have an alt attribute\\n(32:4) \\xe2\\x80\\x93 A11y: <img> element should have an alt attribute\\n(33:4) \\xe2\\x80\\x93 A11y: <img> element should have an alt attribute\\n(39:4) \\xe2\\x80\\x93 A11y: <img> element should have an alt attribute\\n(40:4) \\xe2\\x80\\x93 A11y: <img> element should have an alt attribute\\n(46:4) \\xe2\\x80\\x93 A11y: <img> element should have an alt attribute\\n(47:4) \\xe2\\x80\\x93 A11y: <img> element should have an alt attribute\\n(53:4) \\xe2\\x80\\x93 A11y: <img> element should have an alt attribute\\n(54:4) \\xe2\\x80\\x93 A11y: <img> element should have an alt attribute\\n(60:4) \\xe2\\x80\\x93 A11y: <img> element should have an alt attribute\\n(61:4) \\xe2\\x80\\x93 A11y: <img> element should have an alt attribute\\n'\n"
     ]
    }
   ],
   "source": [
    "%%html_define_svelte SpatialWidget\n",
    "\n",
    "<div class=\"figure\" style=\"width: 10000px; height: 250px; contain: strict;\">\n",
    "  <div class=\"outer\" on:mouseleave=\"set({pos2: undefined})\">\n",
    "    <img class=\"img\"  src=\"{{img}}\">\n",
    "    <img class=\"attr\" src=\"{{(pos1 == undefined)? hint1 : spritemap1[pos1[1]][pos1[0]]}}\">\n",
    "\n",
    "    <div class=\"label\">mixed3a {{pct1}}</div>\n",
    "  </div>\n",
    "\n",
    "  <div class=\"outer\" on:mouseleave=\"set({pos2: undefined})\">\n",
    "    <img class=\"img\"  src=\"{{img}}\">\n",
    "    <img class=\"attr\" src=\"{{(pos1 == undefined)? hint2 : spritemap1[pos1[1]][pos1[0]]}}\">\n",
    "\n",
    "    <div class=\"label\">mixed3b {{pct2}}</div>\n",
    "  </div>  \n",
    "\n",
    "  <div class=\"outer\" on:mouseleave=\"set({pos2: undefined})\">\n",
    "    <img class=\"img\"  src=\"{{img}}\">\n",
    "    <img class=\"attr\" src=\"{{(pos1 == undefined)? hint3 : spritemap1[pos1[1]][pos1[0]]}}\">\n",
    "\n",
    "    <div class=\"label\">mixed4a {{pct3}}</div>\n",
    "  </div>\n",
    "\n",
    "  <div class=\"outer\" on:mouseleave=\"set({pos2: undefined})\">\n",
    "    <img class=\"img\"  src=\"{{img}}\">\n",
    "    <img class=\"attr\" src=\"{{(pos1 == undefined)? hint4 : spritemap1[pos1[1]][pos1[0]]}}\">\n",
    "\n",
    "    <div class=\"label\">mixed4b {{pct4}}</div>\n",
    "  </div>  \n",
    "\n",
    "  <div class=\"outer\" on:mouseleave=\"set({pos2: undefined})\">\n",
    "    <img class=\"img\"  src=\"{{img}}\">\n",
    "    <img class=\"attr\" src=\"{{(pos1 == undefined)? hint5 : spritemap1[pos1[1]][pos1[0]]}}\">\n",
    "\n",
    "    <div class=\"label\">mixed4c {{pct5}}</div>\n",
    "  </div>\n",
    "\n",
    " <div class=\"outer\" on:mouseleave=\"set({pos2: undefined})\">\n",
    "    <img class=\"img\"  src=\"{{img}}\">\n",
    "    <img class=\"attr\" src=\"{{(pos1 == undefined)? hint6 : spritemap1[pos1[1]][pos1[0]]}}\">\n",
    "\n",
    "    <div class=\"label\">mixed4d {{pct6}}</div>\n",
    "  </div>\n",
    "\n",
    "  <div class=\"outer\" on:mouseleave=\"set({pos2: undefined})\">\n",
    "    <img class=\"img\"  src=\"{{img}}\">\n",
    "    <img class=\"attr\" src=\"{{(pos1 == undefined)? hint6 : spritemap1[pos1[1]][pos1[0]]}}\">\n",
    "\n",
    "    <div class=\"label\">mixed4e {{pct7}}</div>\n",
    "  </div>  \n",
    "\n",
    "  <div class=\"outer\" on:mouseleave=\"set({pos2: undefined})\">\n",
    "    <img class=\"img\"  src=\"{{img}}\">\n",
    "    <img class=\"attr\" src=\"{{(pos1 == undefined)? hint8 : spritemap1[pos1[1]][pos1[0]]}}\">\n",
    "\n",
    "    <div class=\"label\">mixed5a {{pct8}}</div>\n",
    "  </div>\n",
    "\n",
    "    <div class=\"outer\" on:mouseleave=\"set({pos2: undefined})\">\n",
    "    <img class=\"img\"  src=\"{{img}}\">\n",
    "    <img class=\"attr\" src=\"{{(pos1 == undefined)? hint9 : spritemap1[pos1[1]][pos1[0]]}}\">\n",
    "\n",
    "    <div class=\"label\">mixed5b {{pct9}}</div>\n",
    "  </div>\n",
    "  \n",
    "</div>\n",
    "\n",
    "\n",
    "<style>\n",
    "\n",
    "  .outer{\n",
    "    width: 224px;\n",
    "    height: 224px;\n",
    "    display: inline-block;\n",
    "    margin-right: 2px;\n",
    "    position: relative;\n",
    "  }\n",
    "  .outer img, .outer svg {\n",
    "    position: absolute;\n",
    "    left: 0px;\n",
    "    top: 0px;\n",
    "    width: 224px;\n",
    "    height: 224px;\n",
    "    image-rendering: pixelated; \n",
    "  }\n",
    "  .attr {\n",
    "    opacity: 0.6;\n",
    "  }\n",
    "  .pointer_container {\n",
    "    z-index: 100;\n",
    "  }\n",
    "  .pointer_container rect {\n",
    "    opacity: 0;\n",
    "  }\n",
    "  .pointer_container .selected  {\n",
    "    opacity: 1;\n",
    "    fill: none;\n",
    "    stroke: hsl(24, 100%, 50%);\n",
    "    stroke-width: 0.1px;\n",
    "  }\n",
    "  .label{\n",
    "    position: absolute;\n",
    "    left: 0px;\n",
    "    top: 224px;\n",
    "    width: 224px;\n",
    "  }\n",
    "</style>\n",
    "\n",
    "<script>\n",
    "  function range(n){\n",
    "    return Array(n).fill().map((_, i) => i);\n",
    "  }\n",
    "  \n",
    "  export default {\n",
    "    data () {\n",
    "      return {\n",
    "        img: \"\",\n",
    "        hint1: \"\",\n",
    "        hint2: \"\",\n",
    "        hint3: \"\",\n",
    "        hint4: \"\",\n",
    "        hint5: \"\",\n",
    "        hint6: \"\",\n",
    "        hint7: \"\",\n",
    "        hint8: \"\",\n",
    "        hint9: \"\",\n",
    "        pct1: \"\",\n",
    "        pct2: \"\",\n",
    "        pct3: \"\",\n",
    "        pct4: \"\",\n",
    "        pct5: \"\",\n",
    "        pct6: \"\",\n",
    "        pct7: \"\",\n",
    "        pct8: \"\",\n",
    "        pct9: \"\",\n",
    "        spritemap1 : \"\",\n",
    "        size1: 1,\n",
    "        spritemap2 : \"\",\n",
    "        size2: 1,\n",
    "        pos1: undefined,\n",
    "        pos2: undefined,\n",
    "        layer1: \"\",\n",
    "        layer2: \"\"\n",
    "      };\n",
    "    },\n",
    "    computed: {\n",
    "      xs1: (size1) => range(size1),\n",
    "      ys1: (size1) => range(size1),\n",
    "      xs2: (size2) => range(size2),\n",
    "      ys2: (size2) => range(size2)\n",
    "    },\n",
    "    helpers: {range}\n",
    "  };\n",
    "</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OrlLGkWxKpmf"
   },
   "outputs": [],
   "source": [
    "def orange_blue(a,b,clip=False):\n",
    "  \"\"\"This is what makes the orange/blue highlights in the saliency map\"\"\"\n",
    "  \n",
    "  # Clip *is* called on each run in the given code\n",
    "  # --This appears to cut down on noise (in purple) in the final output\n",
    "  if clip:\n",
    "    a,b = np.maximum(a,0), np.maximum(b,0)\n",
    "    \n",
    "  # This is all original to what I was given, I don't know what the constants represent\n",
    "  img = np.stack([a, (a + b)/2., b], -1)\n",
    "  img /= 1e-2 + np.abs(img).max()/1.5\n",
    "  img += 0.3  # Brightens the image a tad\n",
    "  img[img>1] = 1 # Truncate oddities\n",
    "  highlight_count = 0\n",
    "  pixel_count = 0\n",
    "  \n",
    "  # Gets the number of pixels that aren't black\n",
    "  # Due to some image brightening, 'black' is considered 0.3\n",
    "  for row in img:\n",
    "    for pixel in row:\n",
    "        pixel_count += 1\n",
    "        for rgbval in pixel:\n",
    "          if rgbval > 0.3:\n",
    "            highlight_count += 1\n",
    "            break\n",
    "  \n",
    "  return [img, highlight_count/pixel_count]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gQ1bysFVHDnL"
   },
   "source": [
    "# Simple Attribution Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_url_grid(grid):\n",
    "  return [[_image_url(img) for img in line] for line in grid ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load conv1_7x7_s2 weights!\n",
      "Load conv1_7x7_s2 biases!\n",
      "Load conv2_3x3_reduce weights!\n",
      "Load conv2_3x3_reduce biases!\n",
      "Load conv2_3x3 weights!\n",
      "Load conv2_3x3 biases!\n",
      "Load inception_3a_1x1 weights!\n",
      "Load inception_3a_1x1 biases!\n",
      "Load inception_3a_3x3_reduce weights!\n",
      "Load inception_3a_3x3_reduce biases!\n",
      "Load inception_3a_3x3 weights!\n",
      "Load inception_3a_3x3 biases!\n",
      "Load inception_3a_5x5_reduce weights!\n",
      "Load inception_3a_5x5_reduce biases!\n",
      "Load inception_3a_5x5 weights!\n",
      "Load inception_3a_5x5 biases!\n",
      "Load inception_3a_pool_proj weights!\n",
      "Load inception_3a_pool_proj biases!\n",
      "Load inception_3b_1x1 weights!\n",
      "Load inception_3b_1x1 biases!\n",
      "Load inception_3b_3x3_reduce weights!\n",
      "Load inception_3b_3x3_reduce biases!\n",
      "Load inception_3b_3x3 weights!\n",
      "Load inception_3b_3x3 biases!\n",
      "Load inception_3b_5x5_reduce weights!\n",
      "Load inception_3b_5x5_reduce biases!\n",
      "Load inception_3b_5x5 weights!\n",
      "Load inception_3b_5x5 biases!\n",
      "Load inception_3b_pool_proj weights!\n",
      "Load inception_3b_pool_proj biases!\n",
      "Load inception_4a_1x1 weights!\n",
      "Load inception_4a_1x1 biases!\n",
      "Load inception_4a_3x3_reduce weights!\n",
      "Load inception_4a_3x3_reduce biases!\n",
      "Load inception_4a_3x3 weights!\n",
      "Load inception_4a_3x3 biases!\n",
      "Load inception_4a_5x5_reduce weights!\n",
      "Load inception_4a_5x5_reduce biases!\n",
      "Load inception_4a_5x5 weights!\n",
      "Load inception_4a_5x5 biases!\n",
      "Load inception_4a_pool_proj weights!\n",
      "Load inception_4a_pool_proj biases!\n",
      "Load inception_4b_1x1 weights!\n",
      "Load inception_4b_1x1 biases!\n",
      "Load inception_4b_3x3_reduce weights!\n",
      "Load inception_4b_3x3_reduce biases!\n",
      "Load inception_4b_3x3 weights!\n",
      "Load inception_4b_3x3 biases!\n",
      "Load inception_4b_5x5_reduce weights!\n",
      "Load inception_4b_5x5_reduce biases!\n",
      "Load inception_4b_5x5 weights!\n",
      "Load inception_4b_5x5 biases!\n",
      "Load inception_4b_pool_proj weights!\n",
      "Load inception_4b_pool_proj biases!\n",
      "Load inception_4c_1x1 weights!\n",
      "Load inception_4c_1x1 biases!\n",
      "Load inception_4c_3x3_reduce weights!\n",
      "Load inception_4c_3x3_reduce biases!\n",
      "Load inception_4c_3x3 weights!\n",
      "Load inception_4c_3x3 biases!\n",
      "Load inception_4c_5x5_reduce weights!\n",
      "Load inception_4c_5x5_reduce biases!\n",
      "Load inception_4c_5x5 weights!\n",
      "Load inception_4c_5x5 biases!\n",
      "Load inception_4c_pool_proj weights!\n",
      "Load inception_4c_pool_proj biases!\n",
      "Load inception_4d_1x1 weights!\n",
      "Load inception_4d_1x1 biases!\n",
      "Load inception_4d_3x3_reduce weights!\n",
      "Load inception_4d_3x3_reduce biases!\n",
      "Load inception_4d_3x3 weights!\n",
      "Load inception_4d_3x3 biases!\n",
      "Load inception_4d_5x5_reduce weights!\n",
      "Load inception_4d_5x5_reduce biases!\n",
      "Load inception_4d_5x5 weights!\n",
      "Load inception_4d_5x5 biases!\n",
      "Load inception_4d_pool_proj weights!\n",
      "Load inception_4d_pool_proj biases!\n",
      "Load inception_4e_1x1 weights!\n",
      "Load inception_4e_1x1 biases!\n",
      "Load inception_4e_3x3_reduce weights!\n",
      "Load inception_4e_3x3_reduce biases!\n",
      "Load inception_4e_3x3 weights!\n",
      "Load inception_4e_3x3 biases!\n",
      "Load inception_4e_5x5_reduce weights!\n",
      "Load inception_4e_5x5_reduce biases!\n",
      "Load inception_4e_5x5 weights!\n",
      "Load inception_4e_5x5 biases!\n",
      "Load inception_4e_pool_proj weights!\n",
      "Load inception_4e_pool_proj biases!\n",
      "Load inception_5a_1x1 weights!\n",
      "Load inception_5a_1x1 biases!\n",
      "Load inception_5a_3x3_reduce weights!\n",
      "Load inception_5a_3x3_reduce biases!\n",
      "Load inception_5a_3x3 weights!\n",
      "Load inception_5a_3x3 biases!\n",
      "Load inception_5a_5x5_reduce weights!\n",
      "Load inception_5a_5x5_reduce biases!\n",
      "Load inception_5a_5x5 weights!\n",
      "Load inception_5a_5x5 biases!\n",
      "Load inception_5a_pool_proj weights!\n",
      "Load inception_5a_pool_proj biases!\n",
      "Load inception_5b_1x1 weights!\n",
      "Load inception_5b_1x1 biases!\n",
      "Load inception_5b_3x3_reduce weights!\n",
      "Load inception_5b_3x3_reduce biases!\n",
      "Load inception_5b_3x3 weights!\n",
      "Load inception_5b_3x3 biases!\n",
      "Load inception_5b_5x5_reduce weights!\n",
      "Load inception_5b_5x5_reduce biases!\n",
      "Load inception_5b_5x5 weights!\n",
      "Load inception_5b_5x5 biases!\n",
      "Load inception_5b_pool_proj weights!\n",
      "Load inception_5b_pool_proj biases!\n",
      "Load loss3_classifier weights!\n",
      "Load loss3_classifier biases!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/w266ajh/.local/lib/python3.6/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/home/w266ajh/.local/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
      "/home/w266ajh/.local/lib/python3.6/site-packages/skimage/util/dtype.py:141: UserWarning: Possible precision loss when converting from float64 to uint8\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: probability: 0.61, label: tabby\n",
      "1: probability: 0.70, label: tabby\n",
      "1: probability: 0.42, label: tiger cat\n",
      "1: probability: 0.42, label: tiger cat\n"
     ]
    }
   ],
   "source": [
    "# This block gets the saliency maps themselves\n",
    "\n",
    "layers = ['mixed3a', 'mixed3b', 'mixed4a', 'mixed4b', 'mixed4c', 'mixed4d', 'mixed4e', 'mixed5a', 'mixed5b']\n",
    "cur_labels = get_top_five(\"/home/w266ajh/Documents/jupyter_notebooks/adslk/\", \".png\")\n",
    "for label in cur_labels:\n",
    "    # Note: Must save the file off each time because it will not re-read raw ndarray\n",
    "    file = label.pop(0)\n",
    "    img = resize(load(file), (224, 224))  # Make all 224x224\n",
    "    imsave(\"current.jpeg\", img, plugin='pil', format_str='jpeg')\n",
    "    img = load(\"current.jpeg\")\n",
    "    for tag in label:\n",
    "        print(tag)\n",
    "        saliency_layers = []\n",
    "        saliency_values = []\n",
    "        for layer in layers:\n",
    "            # Get our saliency layer\n",
    "            saliency = orange_blue(\n",
    "              raw_class_spatial_attr(img, layer, tag.split(':')[-1].strip(), override=None),\n",
    "              raw_class_spatial_attr(img, layer, None, override=None),\n",
    "              clip=True\n",
    "            )\n",
    "            saliency_layers.append(saliency[0])\n",
    "            saliency_values.append(saliency[1])\n",
    "            imsave(\"../saliency_maps/\" + file.split('/')[-1] + '_' + layer + \".png\", saliency[0], plugin='pil', format_str='png')\n",
    "\n",
    "            attrs = raw_spatial_spatial_attr(img, layer, \"mixed5b\", override=None)\n",
    "            attrs = attrs / attrs.max()\n",
    "            break\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_interest_areas(saliency, input_image):\n",
    "    # Load in our image\n",
    "    img = np.array(Image.open(saliency).resize((224,224))) / 255  # Adjust input saliency here\n",
    "    \n",
    "\n",
    "    # Eliminate the low values--leave only high\n",
    "    img[img<0.5] = 0  # Adjust the dropout here\n",
    "\n",
    "    # Save off modified saliency\n",
    "    imsave(\"testing.png\", img, plugin='pil', format_str='png')\n",
    "\n",
    "    ###\n",
    "\n",
    "    # Get our overlay image\n",
    "    img_pastable = Image.open(input_image)  # Adjust input image here\n",
    "    img_pastable = img_pastable.resize((224, 224), Image.ANTIALIAS)\n",
    "    img_overlay = Image.open(\"testing.png\")\n",
    "    img_overlay = img_overlay.resize((224, 224))\n",
    "    img_overlay = img_overlay.convert(\"RGBA\")\n",
    "    datas = img_overlay.getdata()\n",
    "\n",
    "    newData = []\n",
    "    for item in datas:\n",
    "        if item[0] == 0 and item[1] == 0 and item[2] == 0:\n",
    "            newData.append((0, 0, 0, 0))\n",
    "        else:\n",
    "            newData.append(item)\n",
    "\n",
    "    img_overlay.putdata(newData)\n",
    "\n",
    "    img_pastable.paste(img_overlay, (0,0), img_overlay)\n",
    "    img_pastable.save('overlaid', 'png', quality=100)\n",
    "\n",
    "    ###\n",
    "\n",
    "    # Get the overlaid region out of the original image\n",
    "    x, y, z = (img != 0).nonzero()\n",
    "\n",
    "    # Iterate through the photo, using only the things ID'd as not highlighted\n",
    "    photo_chunk = np.zeros((224,224,3))\n",
    "    photo = np.array(Image.open(input_image).resize((224,224), Image.ANTIALIAS)) / 255\t# Adjust input image here\n",
    "    imsave(\"testing_chunks.png\", photo, plugin='pil', format_str='png')\n",
    "\n",
    "    for idx, subx in enumerate(x):\n",
    "      for i in range(0, 3):\n",
    "        photo_chunk[subx, y[idx], i] = photo[subx, y[idx], i]\n",
    "    imsave(\"/home/w266ajh/Documents/output_interests/\" + input_image.split('/')[-1], photo_chunk, plugin='pil', format_str='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "barn1.jpg\n",
      "barn1.jpg_mixed3a.png\n",
      "barn2.jpg\n",
      "barn2.jpg_mixed3a.png\n",
      "barn3.jpg\n",
      "barn3.jpg_mixed3a.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/w266ajh/.local/lib/python3.6/site-packages/skimage/util/dtype.py:141: UserWarning: Possible precision loss when converting from float64 to uint8\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "barn4.jpg\n",
      "barn4.jpg_mixed3a.png\n",
      "barn5.jpg\n",
      "barn5.jpg_mixed3a.png\n",
      "dam1.jpg\n",
      "dam1.jpg_mixed3a.png\n",
      "dam2.jpg\n",
      "dam2.jpg_mixed3a.png\n",
      "dam3.jpg\n",
      "dam3.jpg_mixed3a.png\n",
      "dam4.jpg\n",
      "dam4.jpg_mixed3a.png\n",
      "dam5.jpg\n",
      "dam5.jpg_mixed3a.png\n",
      "fridge1.jpg\n",
      "fridge1.jpg_mixed3a.png\n",
      "fridge2.jpg\n",
      "fridge2.jpg_mixed3a.png\n",
      "fridge3.jpg\n",
      "fridge3.jpg_mixed3a.png\n",
      "fridge4.jpg\n",
      "fridge4.jpg_mixed3a.png\n",
      "fridge5.jpg\n",
      "fridge5.jpg_mixed3a.png\n",
      "gaspump1.jpg\n",
      "gaspump1.jpg_mixed3a.png\n",
      "gaspump2.jpg\n",
      "gaspump2.jpg_mixed3a.png\n",
      "gaspump3.jpg\n",
      "gaspump3.jpg_mixed3a.png\n",
      "gaspump4.jpg\n",
      "gaspump4.jpg_mixed3a.png\n",
      "gaspump5.jpg\n",
      "gaspump5.jpg_mixed3a.png\n",
      "mashedpotato1.jpg\n",
      "mashedpotato1.jpg_mixed3a.png\n",
      "mashedpotato2.jpg\n",
      "mashedpotato2.jpg_mixed3a.png\n",
      "mashedpotato3.jpg\n",
      "mashedpotato3.jpg_mixed3a.png\n",
      "mashedpotato4.jpg\n",
      "mashedpotato4.jpg_mixed3a.png\n",
      "mashedpotato5.jpg\n",
      "mashedpotato5.jpg_mixed3a.png\n",
      "pier1.jpg\n",
      "pier1.jpg_mixed3a.png\n",
      "pier2.jpg\n",
      "pier2.jpg_mixed3a.png\n",
      "pier3.jpg\n",
      "pier3.jpg_mixed3a.png\n",
      "pier4.jpg\n",
      "pier4.jpg_mixed3a.png\n",
      "pier5.jpg\n",
      "pier5.jpg_mixed3a.png\n",
      "polaroid1.jpg\n",
      "polaroid1.jpg_mixed3a.png\n",
      "polaroid2.jpg\n",
      "polaroid2.jpg_mixed3a.png\n",
      "polaroid3.jpg\n",
      "polaroid3.jpg_mixed3a.png\n",
      "polaroid4.jpg\n",
      "polaroid4.jpg_mixed3a.png\n",
      "polaroid5.jpg\n",
      "polaroid5.jpg_mixed3a.png\n",
      "sharpener1.jpg\n",
      "sharpener1.jpg_mixed3a.png\n",
      "sharpener2.jpg\n",
      "sharpener2.jpg_mixed3a.png\n",
      "sharpener3.jpg\n",
      "sharpener3.jpg_mixed3a.png\n",
      "sharpener4.jpg\n",
      "sharpener4.jpg_mixed3a.png\n",
      "sharpener5.jpg\n",
      "sharpener5.jpg_mixed3a.png\n",
      "telephone1.jpg\n",
      "telephone1.jpg_mixed3a.png\n",
      "telephone2.jpg\n",
      "telephone2.jpg_mixed3a.png\n",
      "telephone3.jpg\n",
      "telephone3.jpg_mixed3a.png\n",
      "telephone4.jpg\n",
      "telephone4.jpg_mixed3a.png\n",
      "telephone5.jpg\n",
      "telephone5.jpg_mixed3a.png\n",
      "watertower1.jpg\n",
      "watertower1.jpg_mixed3a.png\n",
      "watertower2.jpg\n",
      "watertower2.jpg_mixed3a.png\n",
      "watertower3.jpg\n",
      "watertower3.jpg_mixed3a.png\n",
      "watertower4.jpg\n",
      "watertower4.jpg_mixed3a.png\n",
      "watertower5.jpg\n",
      "watertower5.jpg_mixed3a.png\n"
     ]
    }
   ],
   "source": [
    "root_img_path = \"/home/w266ajh/Documents/top5gen/data_imageparts\"\n",
    "root_sal_path = \"/home/w266ajh/Documents/saliency_maps\"\n",
    "image_list = sorted([image for image in listdir(root_img_path) if image.endswith(\".jpg\")])\n",
    "saliency_list = sorted([sal for sal in listdir(root_sal_path)])\n",
    "for idx, img in enumerate(image_list):\n",
    "    print(img)\n",
    "    print(saliency_list[idx])\n",
    "    highlight_interest_areas(root_sal_path + '/' + saliency_list[idx], root_img_path + '/' + img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Spatial Attribution - Building Blocks of Interpretability",
   "provenance": [
    {
     "file_id": "https://github.com/tensorflow/lucid/blob/master/notebooks/building-blocks/AttrSpatial.ipynb",
     "timestamp": 1539051212813
    },
    {
     "file_id": "1uRqpBNPg-aW3tRU-uo-mWg6cQxuAquHW",
     "timestamp": 1518822563463
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
