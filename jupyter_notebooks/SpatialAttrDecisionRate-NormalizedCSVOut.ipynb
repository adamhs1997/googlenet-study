{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JndnmDMp66FL"
   },
   "source": [
    "##### Copyright 2018 Google LLC.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "hMqWDc_m6rUC"
   },
   "outputs": [],
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hOBBuzMaxU37"
   },
   "source": [
    "# Install / Import / Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UL1yOZtjqkcj"
   },
   "source": [
    "This code depends on [Lucid](https://github.com/tensorflow/lucid) (our visualization library), and [svelte](https://svelte.technology/) (a web framework). The following cell will install both of them, and dependancies such as TensorFlow. And then import them as appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15298,
     "status": "ok",
     "timestamp": 1539449536510,
     "user": {
      "displayName": "Adam Horvath-Smith",
      "photoUrl": "",
      "userId": "02503422148139327677"
     },
     "user_tz": 240
    },
    "id": "AA17rJBLuyYH",
    "outputId": "4107bb2a-9a1a-44cd-d97e-42049a84cd8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/w266ajh/.local/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:104: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`normal` is a deprecated alias for `truncated_normal`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/w266ajh/.local/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:104: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`normal` is a deprecated alias for `truncated_normal`\n"
     ]
    }
   ],
   "source": [
    "#!npm install -g svelte-cli@2.2.0\n",
    "#!pip install --user scikit-image\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imsave\n",
    "\n",
    "import lucid.modelzoo.vision_models as models\n",
    "from lucid.misc.io import show\n",
    "import lucid.optvis.render as render\n",
    "from lucid.misc.io import show, load\n",
    "from lucid.misc.io.reading import read\n",
    "from lucid.misc.io.showing import _image_url\n",
    "from lucid.misc.gradient_override import gradient_override_map\n",
    "import lucid.scratch.web.svelte as lucid_svelte\n",
    "\n",
    "from top5gen.examples.inception_pretrained import get_top_five"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0cUPBCRyG9xE"
   },
   "source": [
    "# Attribution Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FWHqimIqk2Bs"
   },
   "outputs": [],
   "source": [
    "# This, obviously, loads up the model\n",
    "model = models.InceptionV1()\n",
    "model.load_graphdef()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 381,
     "status": "ok",
     "timestamp": 1539452363680,
     "user": {
      "displayName": "Adam Horvath-Smith",
      "photoUrl": "",
      "userId": "02503422148139327677"
     },
     "user_tz": 240
    },
    "id": "xIDcG0vjaDtk",
    "outputId": "c0b7cb0b-d043-45ee-acf6-90875a5cd584"
   },
   "outputs": [],
   "source": [
    "# Here we take in the 1000-dimension vector of potential image labels\n",
    "labels_str = read(\"https://gist.githubusercontent.com/aaronpolhamus/964a4411c0906315deb9f4a3723aac57/raw/aa66dd9dbf6b56649fa3fab83659b2acbf3cbfd1/map_clsloc.txt\")\n",
    "labels = [line[line.find(\" \"):].strip().encode() for line in labels_str.decode().split(\"\\n\")]\n",
    "labels = [label.decode().strip().split()[1].replace(\"_\", \" \") for label in labels]\n",
    "labels = [\"dummy\"] + labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p1S73WcbKIdI"
   },
   "outputs": [],
   "source": [
    "def raw_class_spatial_attr(img, layer, label, override=None):\n",
    "  \"\"\"How much did spatial positions at a given layer effect a output class?\"\"\"\n",
    "\n",
    "  # Set up a graph for doing attribution...\n",
    "  with tf.Graph().as_default(), tf.Session(), gradient_override_map(override or {}):\n",
    "    t_input = tf.placeholder_with_default(img, [None, None, 3])\n",
    "    T = render.import_model(model, t_input, t_input)\n",
    "    \n",
    "    # Compute activations\n",
    "    acts = T(layer).eval()\n",
    "    \n",
    "    # display results\n",
    "\n",
    "    \n",
    "    if label is None: return np.zeros(acts.shape[1:-1])\n",
    "    \n",
    "    # Compute gradient\n",
    "    softmax_result = T(\"softmax2_pre_activation\")\n",
    "    score = softmax_result[0, labels.index(label)] # The \"score\" is the softmax result at idx [0, label idx]\n",
    "    t_grad = tf.gradients([score], [T(layer)])[0]   \n",
    "    grad = t_grad.eval({T(layer) : acts})\n",
    "    \n",
    "    # Linear approximation of effect of spatial position\n",
    "    return np.sum(acts * grad, -1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v_jkx5Niji4Q"
   },
   "outputs": [],
   "source": [
    "def raw_spatial_spatial_attr(img, layer1, layer2, override=None):\n",
    "  \"\"\"Attribution between spatial positions in two different layers.\"\"\"\n",
    "\n",
    "  # Set up a graph for doing attribution...\n",
    "  with tf.Graph().as_default(), tf.Session(), gradient_override_map(override or {}):\n",
    "    t_input = tf.placeholder_with_default(img, [None, None, 3])\n",
    "    T = render.import_model(model, t_input, t_input)\n",
    "    \n",
    "    # Compute activations\n",
    "    acts1 = T(layer1).eval()\n",
    "    acts2 = T(layer2).eval({T(layer1) : acts1})\n",
    "    \n",
    "    # Construct gradient tensor\n",
    "    # Backprop from spatial position (n_x, n_y) in layer2 to layer1.\n",
    "    n_x, n_y = tf.placeholder(\"int32\", []), tf.placeholder(\"int32\", [])\n",
    "    layer2_mags = tf.sqrt(tf.reduce_sum(T(layer2)**2, -1))[0]\n",
    "    score = layer2_mags[n_x, n_y]\n",
    "    t_grad = tf.gradients([score], [T(layer1)])[0]\n",
    "    \n",
    "    # Compute attribution backwards from each positin in layer2\n",
    "    attrs = []\n",
    "    for i in range(acts2.shape[1]):\n",
    "      attrs_ = []\n",
    "      for j in range(acts2.shape[2]):\n",
    "        grad = t_grad.eval({n_x : i, n_y : j, T(layer1) : acts1})\n",
    "        # linear approximation of imapct\n",
    "        attr = np.sum(acts1 * grad, -1)[0]\n",
    "        attrs_.append(attr)\n",
    "      attrs.append(attrs_)\n",
    "  return np.asarray(attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OrlLGkWxKpmf"
   },
   "outputs": [],
   "source": [
    "def orange_blue(a,b,clip=False):\n",
    "  \"\"\"This is what makes the orange/blue highlights in the saliency map\"\"\"\n",
    "  \n",
    "  # Clip *is* called on each run in the given code\n",
    "  # --This appears to cut down on noise (in purple) in the final output\n",
    "  if clip:\n",
    "    a,b = np.maximum(a,0), np.maximum(b,0)\n",
    "    \n",
    "  # This is all original to what I was given, I don't know what the constants represent\n",
    "  arr = np.stack([a, (a + b)/2., b], -1)\n",
    "  arr /= 1e-2 + np.abs(arr).max()/1.5\n",
    "  arr += 0.3  # Brightens the image a tad\n",
    "  i = 0\n",
    "  j = 0\n",
    "  \n",
    "  for values in arr:\n",
    "    for value in values:\n",
    "        for a in value:\n",
    "          if a == 0.3: i += 1\n",
    "          j += 1\n",
    "  \n",
    "  return [arr, i/j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gQ1bysFVHDnL"
   },
   "source": [
    "# Simple Attribution Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load conv1_7x7_s2 weights!\n",
      "Load conv1_7x7_s2 biases!\n",
      "Load conv2_3x3_reduce weights!\n",
      "Load conv2_3x3_reduce biases!\n",
      "Load conv2_3x3 weights!\n",
      "Load conv2_3x3 biases!\n",
      "Load inception_3a_1x1 weights!\n",
      "Load inception_3a_1x1 biases!\n",
      "Load inception_3a_3x3_reduce weights!\n",
      "Load inception_3a_3x3_reduce biases!\n",
      "Load inception_3a_3x3 weights!\n",
      "Load inception_3a_3x3 biases!\n",
      "Load inception_3a_5x5_reduce weights!\n",
      "Load inception_3a_5x5_reduce biases!\n",
      "Load inception_3a_5x5 weights!\n",
      "Load inception_3a_5x5 biases!\n",
      "Load inception_3a_pool_proj weights!\n",
      "Load inception_3a_pool_proj biases!\n",
      "Load inception_3b_1x1 weights!\n",
      "Load inception_3b_1x1 biases!\n",
      "Load inception_3b_3x3_reduce weights!\n",
      "Load inception_3b_3x3_reduce biases!\n",
      "Load inception_3b_3x3 weights!\n",
      "Load inception_3b_3x3 biases!\n",
      "Load inception_3b_5x5_reduce weights!\n",
      "Load inception_3b_5x5_reduce biases!\n",
      "Load inception_3b_5x5 weights!\n",
      "Load inception_3b_5x5 biases!\n",
      "Load inception_3b_pool_proj weights!\n",
      "Load inception_3b_pool_proj biases!\n",
      "Load inception_4a_1x1 weights!\n",
      "Load inception_4a_1x1 biases!\n",
      "Load inception_4a_3x3_reduce weights!\n",
      "Load inception_4a_3x3_reduce biases!\n",
      "Load inception_4a_3x3 weights!\n",
      "Load inception_4a_3x3 biases!\n",
      "Load inception_4a_5x5_reduce weights!\n",
      "Load inception_4a_5x5_reduce biases!\n",
      "Load inception_4a_5x5 weights!\n",
      "Load inception_4a_5x5 biases!\n",
      "Load inception_4a_pool_proj weights!\n",
      "Load inception_4a_pool_proj biases!\n",
      "Load inception_4b_1x1 weights!\n",
      "Load inception_4b_1x1 biases!\n",
      "Load inception_4b_3x3_reduce weights!\n",
      "Load inception_4b_3x3_reduce biases!\n",
      "Load inception_4b_3x3 weights!\n",
      "Load inception_4b_3x3 biases!\n",
      "Load inception_4b_5x5_reduce weights!\n",
      "Load inception_4b_5x5_reduce biases!\n",
      "Load inception_4b_5x5 weights!\n",
      "Load inception_4b_5x5 biases!\n",
      "Load inception_4b_pool_proj weights!\n",
      "Load inception_4b_pool_proj biases!\n",
      "Load inception_4c_1x1 weights!\n",
      "Load inception_4c_1x1 biases!\n",
      "Load inception_4c_3x3_reduce weights!\n",
      "Load inception_4c_3x3_reduce biases!\n",
      "Load inception_4c_3x3 weights!\n",
      "Load inception_4c_3x3 biases!\n",
      "Load inception_4c_5x5_reduce weights!\n",
      "Load inception_4c_5x5_reduce biases!\n",
      "Load inception_4c_5x5 weights!\n",
      "Load inception_4c_5x5 biases!\n",
      "Load inception_4c_pool_proj weights!\n",
      "Load inception_4c_pool_proj biases!\n",
      "Load inception_4d_1x1 weights!\n",
      "Load inception_4d_1x1 biases!\n",
      "Load inception_4d_3x3_reduce weights!\n",
      "Load inception_4d_3x3_reduce biases!\n",
      "Load inception_4d_3x3 weights!\n",
      "Load inception_4d_3x3 biases!\n",
      "Load inception_4d_5x5_reduce weights!\n",
      "Load inception_4d_5x5_reduce biases!\n",
      "Load inception_4d_5x5 weights!\n",
      "Load inception_4d_5x5 biases!\n",
      "Load inception_4d_pool_proj weights!\n",
      "Load inception_4d_pool_proj biases!\n",
      "Load inception_4e_1x1 weights!\n",
      "Load inception_4e_1x1 biases!\n",
      "Load inception_4e_3x3_reduce weights!\n",
      "Load inception_4e_3x3_reduce biases!\n",
      "Load inception_4e_3x3 weights!\n",
      "Load inception_4e_3x3 biases!\n",
      "Load inception_4e_5x5_reduce weights!\n",
      "Load inception_4e_5x5_reduce biases!\n",
      "Load inception_4e_5x5 weights!\n",
      "Load inception_4e_5x5 biases!\n",
      "Load inception_4e_pool_proj weights!\n",
      "Load inception_4e_pool_proj biases!\n",
      "Load inception_5a_1x1 weights!\n",
      "Load inception_5a_1x1 biases!\n",
      "Load inception_5a_3x3_reduce weights!\n",
      "Load inception_5a_3x3_reduce biases!\n",
      "Load inception_5a_3x3 weights!\n",
      "Load inception_5a_3x3 biases!\n",
      "Load inception_5a_5x5_reduce weights!\n",
      "Load inception_5a_5x5_reduce biases!\n",
      "Load inception_5a_5x5 weights!\n",
      "Load inception_5a_5x5 biases!\n",
      "Load inception_5a_pool_proj weights!\n",
      "Load inception_5a_pool_proj biases!\n",
      "Load inception_5b_1x1 weights!\n",
      "Load inception_5b_1x1 biases!\n",
      "Load inception_5b_3x3_reduce weights!\n",
      "Load inception_5b_3x3_reduce biases!\n",
      "Load inception_5b_3x3 weights!\n",
      "Load inception_5b_3x3 biases!\n",
      "Load inception_5b_5x5_reduce weights!\n",
      "Load inception_5b_5x5_reduce biases!\n",
      "Load inception_5b_5x5 weights!\n",
      "Load inception_5b_5x5 biases!\n",
      "Load inception_5b_pool_proj weights!\n",
      "Load inception_5b_pool_proj biases!\n",
      "Load loss3_classifier weights!\n",
      "Load loss3_classifier biases!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/w266ajh/.local/lib/python3.6/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/home/w266ajh/.local/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
      "/home/w266ajh/.local/lib/python3.6/site-packages/skimage/util/dtype.py:141: UserWarning: Possible precision loss when converting from float64 to uint8\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    }
   ],
   "source": [
    "layers = ['mixed3a', 'mixed3b', 'mixed4a', 'mixed4b', 'mixed4c', 'mixed4d', 'mixed4e', 'mixed5a', 'mixed5b']\n",
    "\n",
    "# Write the labels out to the file first\n",
    "with open (\"normalized_file_data.csv\", \"w\") as out:\n",
    "    out.write(\",,,'mixed3a','mixed3b','mixed4a','mixed4b','mixed4c','mixed4d','mixed4e','mixed5a','mixed5b'\\n\")\n",
    "    \n",
    "# Get our top five preds\n",
    "cur_labels = get_top_five()\n",
    "\n",
    "# Hold the current file name\n",
    "file_name = \"\"\n",
    "\n",
    "for label in cur_labels:\n",
    "    # Hold the file path name\n",
    "    file_name = label[0]\n",
    "    \n",
    "    # Note: Must save the file off each time because it will not re-read raw ndarray\n",
    "    img = resize(load(label.pop(0)), (224, 224))  # Make all 224x224\n",
    "    imsave(\"current.jpeg\", img, plugin='pil', format_str='jpeg')\n",
    "    img = load(\"current.jpeg\")\n",
    "    for tag in label:\n",
    "        # Print confidence and the label associated\n",
    "        with open(\"normalized_file_data.csv\", 'a') as out:\n",
    "            out.write(file_name + \",\")  # File name\n",
    "            out.write(tag.split()[2].strip()[:-1] + \",\")  # prob\n",
    "            out.write(tag.split(':')[-1].strip() + \",\")  # class\n",
    "            \n",
    "        saliency_layers = []\n",
    "        saliency_values = []\n",
    "        for layer in layers:       \n",
    "            # Get our saliency layer\n",
    "            saliency = orange_blue(\n",
    "              raw_class_spatial_attr(img, layer, tag.split(':')[-1].strip(), override=None),\n",
    "              raw_class_spatial_attr(img, layer, None, override=None),\n",
    "              clip=True\n",
    "            )\n",
    "            saliency_layers.append(saliency[0])\n",
    "            saliency_values.append(saliency[1])\n",
    "\n",
    "            attrs = raw_spatial_spatial_attr(img, layer, \"mixed5b\", override=None)\n",
    "            attrs = attrs / attrs.max()\n",
    "            \n",
    "        # Add each saliency value to the file\n",
    "        with open(\"normalized_file_data.csv\", 'a') as out:\n",
    "            # Normalize the saliency values list\n",
    "            saliency_values = list(map(lambda sv: 100 - sv * 100, saliency_values))\n",
    "            max_cov = max(saliency_values)\n",
    "            min_cov = min(saliency_values)\n",
    "            saliency_values = list(map(lambda sv: (sv - min_cov) / (max_cov - min_cov), saliency_values))\n",
    "            for sv in saliency_values:\n",
    "                out.write(str(sv) + \",\")\n",
    "            out.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Spatial Attribution - Building Blocks of Interpretability",
   "provenance": [
    {
     "file_id": "https://github.com/tensorflow/lucid/blob/master/notebooks/building-blocks/AttrSpatial.ipynb",
     "timestamp": 1539051212813
    },
    {
     "file_id": "1uRqpBNPg-aW3tRU-uo-mWg6cQxuAquHW",
     "timestamp": 1518822563463
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
